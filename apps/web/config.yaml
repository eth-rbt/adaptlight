# AdaptLight Web Configuration

# Brain settings
brain:
  mode: agent
  model: claude-sonnet-4-5-20250929 #claude-haiku-4-5-20251001
  prompt_variant: examples
  max_turns: 10
  verbose: false

# State representation settings
# Options: "original", "pure_python", "stdlib", "stdlib_js"
# - original: r/g/b expression strings with frame variable
# - pure_python: full Python code with math module
# - stdlib: Python code with helper functions (hsv, lerp, ease_in, etc.)
# - stdlib_js: JavaScript code with helper functions (RECOMMENDED - runs natively in browser)
representation:
  version: stdlib_js # original, pure_python, stdlib, stdlib_js

# API keys (loaded from environment or set here)
anthropic:
  api_key: ${ANTHROPIC_API_KEY}

openai:
  api_key: ${OPENAI_API_KEY}

# Supabase (loaded from environment)
supabase:
  url: ${SUPABASE_URL}
  anon_key: ${SUPABASE_ANON_KEY}

# Server settings
server:
  host: 0.0.0.0
  port: 3000
  debug: false

# Storage
storage:
  dir: data/storage

# Web camera + VLM runtime
vision:
  enabled: true               # Set true to allow /api/vision/* endpoints
  mode: realtime              # polling | realtime (streaming path with fallback)
  latest_frame_only: true     # Keep only newest frame while processing to avoid backlog
  model: gpt-4o-mini          # Fast OpenAI vision model
  interval_ms: 2000           # Default VLM interval (>=2s)
  cooldown_ms: 1500           # Minimum event cooldown
  min_confidence: 0.55        # Default confidence threshold for emitting events
  max_image_chars: 2500000    # Guardrail on data URL payload size
  cv:
    enabled: true             # Enable CV package path (OpenCV / pose detector)
    interval_ms: 1000         # Default CV interval (>=1s)
    detector: opencv_hog      # opencv_hog | opencv_face | opencv_motion | posenet
    pose_model_asset: data/models/pose_landmarker_lite.task  # Required when using posenet via MediaPipe Tasks API
    # Optional higher-quality preset:
    # pose_model_asset: data/models/pose_landmarker_full.task
