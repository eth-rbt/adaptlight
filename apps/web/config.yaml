# AdaptLight Web Configuration

# Brain settings
brain:
  mode: agent
  model: claude-sonnet-4-5-20250929 #claude-haiku-4-5-20251001
  prompt_variant: examples
  max_turns: 10
  verbose: false

# State representation settings
# Options: "original", "pure_python", "stdlib", "stdlib_js"
# - original: r/g/b expression strings with frame variable
# - pure_python: full Python code with math module
# - stdlib: Python code with helper functions (hsv, lerp, ease_in, etc.)
# - stdlib_js: JavaScript code with helper functions (RECOMMENDED - runs natively in browser)
representation:
  version: stdlib_js # original, pure_python, stdlib, stdlib_js

# API keys (loaded from environment or set here)
anthropic:
  api_key: ${ANTHROPIC_API_KEY}

openai:
  api_key: ${OPENAI_API_KEY}

# Supabase (loaded from environment)
supabase:
  url: ${SUPABASE_URL}
  anon_key: ${SUPABASE_ANON_KEY}

# Server settings
server:
  host: 0.0.0.0
  port: 3000
  debug: false

# Storage
storage:
  dir: data/storage

# Web camera + VLM runtime
vision:
  enabled: true               # Set true to allow /api/vision/* endpoints
  mode: realtime              # polling | realtime (streaming path with fallback)
  latest_frame_only: true     # Keep only newest frame while processing to avoid backlog
  interval_ms: 2000           # Default VLM interval (>=2s)
  cooldown_ms: 1500           # Minimum event cooldown
  max_image_chars: 2500000    # Guardrail on data URL payload size
  cv:
    enabled: true             # Enable CV detectors (local OpenCV)
    interval_ms: 1000         # Default CV interval (>=1s)
    detector: opencv_hog      # opencv_hog | opencv_face | opencv_motion
  vlm:
    enabled: true             # Enable VLM engine (requires OpenAI API)
    model: gpt-4o-mini        # Fast OpenAI vision model
    min_confidence: 0.55      # Confidence threshold for emitting events

# Audio LLM watcher runtime
# Two modes:
#   - transcript: audio → browser transcription → GPT-4o-mini text analysis (default)
#   - direct: audio → GPT-4o audio model (no transcription, faster but more API cost)
audio:
  enabled: true
  mode: transcript               # 'transcript' or 'direct'
  model: gpt-4o-mini             # Model for text analysis (transcript mode)
  direct_audio_model: gpt-audio-mini-2025-12-15  # Model for direct audio (direct mode)
  interval_ms: 3000
  cooldown_ms: 1500
  max_transcript_chars: 20000
  max_audio_bytes: 5000000       # Max audio size in bytes (direct mode, ~5MB)
  allow_fallback_transcript: false

# Volume watcher runtime
volume:
  enabled: true
  interval_ms: 80
  smoothing_alpha: 0.35
  floor: 0.0
  ceiling: 1.0
