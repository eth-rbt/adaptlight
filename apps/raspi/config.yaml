# AdaptLight RASPi Configuration

# Device identity (for Supabase logging)
device:
  id: lamp1  # Change this for each device (lamp1, lamp2, etc.)

# Brain settings (passed to Brain())
brain:
  mode: agent                    # 'agent' (Claude multi-turn) or 'parser' (OpenAI single-shot)
  model: claude-sonnet-4-5-20250929 #claude-haiku-4-5-20251001
  prompt_variant: examples       # 'concise' or 'examples'
  max_turns: 10
  verbose: false

# State representation settings
# Options: "original", "pure_python", "stdlib"
# - original: r/g/b expression strings with frame variable
# - pure_python: full Python code with math module
# - stdlib: Python code with helper functions (hsv, lerp, ease_in, etc.) - RECOMMENDED for Pi
representation:
  version: stdlib

# API keys (loaded from environment or set here)
anthropic:
  api_key: ${ANTHROPIC_API_KEY}

openai:
  api_key: ${OPENAI_API_KEY}

replicate:
  api_token: ${REPLICATE_API_TOKEN}

# Hardware (RASPi-specific)
hardware:
  led_type: cob_serial           # 'neopixel', 'cob' (GPIO PWM), or 'cob_serial' (Arduino)
  led_count: 16
  led_brightness: 0.3
  button_pin: 23
  record_button_pin: 17
  led_pin: 2

  # Feedback buttons (Yes/No for Supabase logging)
  feedback_yes_pin: 24
  feedback_no_pin: 25


  # COB LED GPIO PWM pins (for led_type: cob)
  cob_red_pin: 12
  cob_green_pin: 13
  cob_blue_pin: 19
  cob_max_duty_cycle: 2.0
  cob_pwm_frequency: 1000

  # COB LED Serial settings (for led_type: cob_serial)
  cob_serial_port: /dev/ttyAMA0
  cob_serial_baudrate: 115200

# Reactive lights (NeoPixel for voice feedback)
reactive_lights:
  enabled: true
  type: neopixel
  led_count: 35
  spi_bus: 0                       # 0 = SPI0 (GPIO10), 1 = SPI1 (GPIO20)
  brightness: 0.5

  # Voice reactive sensitivity settings
  min_amplitude: 5000             # Noise floor (below this = silence)
  max_amplitude: 30000             # Max RMS for full brightness (lower = more sensitive)
  smoothing_alpha: 0.25            # Smoothing (0.1 = smooth/slow, 0.5 = responsive/jittery)

# Voice input
voice:
  enabled: true
  stt_provider: replicate        # 'whisper' or 'replicate'

# Speaker hardware
speaker:
  type: i2s                      # 'usb', 'i2s', or 'hdmi'
  device: plughw:3,0             # ALSA device (MAX98357A is card 3)
  volume: 4.0                    # Volume multiplier (1.0 = normal, 2.0 = 2x louder)
  # For MAX98357A I2S DAC: uses GPIO 18 (BCLK), GPIO 19 (LRCLK), GPIO 21 (DIN)

# Text-to-speech output
speech:
  enabled: true
  mode: default                  # 'default' (TTS after done) or 'parallel' (plan-first, faster TTS)
  provider: openai               # 'openai' or 'edge'
  voice: nova                    # OpenAI: alloy, echo, fable, onyx, nova, shimmer
  instructions: |
    Your response message will be spoken aloud. Keep it under 1 sentences.
    VERY VEYR IMPORTANT:Be direct. Explain what you did technially. Explaing only the crucial information.
    KEEP IT UNDER 1 SENTENCE.
    Example: "Done, I've set it to warm orange" or "Party mode is on, now it will change colors", "stock up is red, down is green, yellow means no change."

# Button timing
button:
  double_click_threshold: 0.3
  hold_threshold: 0.5
  bounce_time: 0.05

# Logging
logging:
  enabled: true
  log_dir: data/logs
  aws_enabled: false

# State machine
state_machine:
  default_state: off
  save_state: true
  state_file: data/state.json

# Storage
storage:
  dir: data/storage              # Directory for memory and pipelines

# Vision runtime (camera + CV/VLM processing)
# Requires: Pi Camera Module (ribbon cable) or USB webcam
vision:
  enabled: true                  # Set true to enable vision features
  camera_index: 0                # Camera device index (0 = Pi Camera via ribbon)
  camera_width: 320              # Capture width (lower = faster CV on Pi)
  camera_height: 240             # Capture height (320x240 recommended for CV)
  mode: polling                  # polling | realtime
  latest_frame_only: true        # Keep only newest frame while processing
  interval_ms: 2000              # Default VLM interval (>=2s, slower but semantic)
  cooldown_ms: 1500              # Minimum time between emitting same event
  max_image_chars: 2500000       # Max base64 data URL size
  cv:
    enabled: false                # Enable CV detectors (local, fast, no API needed)
    interval_ms: 200             # Default CV interval (fast, local processing)
    # Available detectors:
    #   - opencv_hog: person/body detection (recommended)
    #   - opencv_face: face detection
    #   - opencv_motion: motion score 0-1
  vlm:
    enabled: true               # Enable VLM engine (requires OpenAI API, slower)
    model: gpt-4o-mini           # OpenAI vision model
    min_confidence: 0.55         # Confidence threshold for VLM events

# API runtime (api-reactive polling)
api:
  enabled: true                  # Enable API reactive features
  tick_interval_ms: 1000         # How often main loop checks for due fetches
  default_interval_ms: 30000     # Default API poll interval if state doesn't specify
  min_interval_ms: 1000          # Minimum allowed interval (rate limit protection)
  timeout: 15.0                  # HTTP request timeout in seconds
  cooldown_ms: 1000              # Minimum time between emitting same event
