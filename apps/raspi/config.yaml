# AdaptLight RASPi Configuration

# Device identity (for Supabase logging)
device:
  id: lamp1  # Change this for each device (lamp1, lamp2, etc.)

# Brain settings (passed to Brain())
brain:
  mode: agent                    # 'agent' (Claude multi-turn) or 'parser' (OpenAI single-shot)
  model: claude-sonnet-4-5-20250929 #claude-haiku-4-5-20251001
  prompt_variant: examples       # 'concise' or 'examples'
  max_turns: 10
  verbose: false

# State representation settings
# Options: "original", "pure_python", "stdlib"
# - original: r/g/b expression strings with frame variable
# - pure_python: full Python code with math module
# - stdlib: Python code with helper functions (hsv, lerp, ease_in, etc.) - RECOMMENDED for Pi
representation:
  version: stdlib

# API keys (loaded from environment or set here)
anthropic:
  api_key: ${ANTHROPIC_API_KEY}

openai:
  api_key: ${OPENAI_API_KEY}

replicate:
  api_token: ${REPLICATE_API_TOKEN}

# Hardware (RASPi-specific)
hardware:
  led_type: cob_serial           # 'neopixel', 'cob' (GPIO PWM), or 'cob_serial' (Arduino)
  led_count: 16
  led_brightness: 0.3
  button_pin: 23
  record_button_pin: 17
  led_pin: 2

  # Feedback buttons (Yes/No for Supabase logging)
  feedback_yes_pin: 12
  feedback_no_pin: 24


  # COB LED GPIO PWM pins (for led_type: cob)
  cob_red_pin: 16
  cob_green_pin: 13
  cob_blue_pin: 19
  cob_max_duty_cycle: 2.0
  cob_pwm_frequency: 1000

  # COB LED Serial settings (for led_type: cob_serial)
  cob_serial_port: /dev/ttyAMA0
  cob_serial_baudrate: 115200

# Reactive lights (NeoPixel for voice feedback)
reactive_lights:
  enabled: true
  type: neopixel
  led_count: 45
  spi_bus: 0                       # 0 = SPI0 (GPIO10), 1 = SPI1 (GPIO20)
  brightness: 0.5

  # Voice reactive sensitivity settings
  min_amplitude: 2000             # Noise floor (below this = silence)
  max_amplitude: 10000             # Max RMS for full brightness (lower = more sensitive)
  smoothing_alpha: 0.25            # Smoothing (0.1 = smooth/slow, 0.5 = responsive/jittery)

# Voice input
voice:
  enabled: true
  stt_provider: replicate        # 'whisper' or 'replicate'

# Speaker hardware
speaker:
  type: i2s                      # 'usb', 'i2s', or 'hdmi'
  device: plughw:3,0             # ALSA device (MAX98357A is card 3)
  volume: 4.0                    # Volume multiplier (1.0 = normal, 2.0 = 2x louder)
  # For MAX98357A I2S DAC: uses GPIO 18 (BCLK), GPIO 19 (LRCLK), GPIO 21 (DIN)

# Text-to-speech output
speech:
  enabled: true
  mode: default                  # 'default' (TTS after done) or 'parallel' (plan-first, faster TTS)
  provider: openai               # 'openai' or 'edge'
  voice: nova                    # OpenAI: alloy, echo, fable, onyx, nova, shimmer
  instructions: |
    Your response message will be spoken aloud. Keep in 1 sentences!!! ONLY ONE SENTENCE!!! ONLY ONE!!!
    VERY VERY IMPORTANT:Be direct. Explain what you did technially. Explaing only the crucial information.
    KEEP IT UNDER 1 SENTENCE. 
    Example: "Done, I've set it to warm orange" or "Party mode is on, now it will change colors", "stock up is red, down is green, yellow means no change."

# Button timing
button:
  double_click_threshold: 0.3
  hold_threshold: 0.5
  bounce_time: 0.05

# Logging
logging:
  enabled: true
  log_dir: data/logs
  aws_enabled: false

# State machine
state_machine:
  default_state: off
  save_state: true
  state_file: data/state.json

# Storage
storage:
  dir: data/storage              # Directory for memory and pipelines

# Vision runtime (camera + CV/VLM processing)
# Requires: Pi Camera Module (ribbon cable) or USB webcam
vision:
  enabled: true                  # Set true to enable vision features
  camera_index: 0                # Camera device index (0 = Pi Camera via ribbon)
  camera_width: 320              # Capture width (lower = faster CV on Pi)
  camera_height: 240             # Capture height (320x240 recommended for CV)
  mode: polling                  # polling | realtime
  latest_frame_only: true        # Keep only newest frame while processing
  interval_ms: 2000              # Default VLM interval (>=2s, slower but semantic)
  cooldown_ms: 1500              # Minimum time between emitting same event
  max_image_chars: 2500000       # Max base64 data URL size
  cv:
    enabled: false                # Enable CV detectors (local, fast, no API needed)
    interval_ms: 200             # Default CV interval (fast, local processing)
    # Available detectors:
    #   - opencv_hog: person/body detection (recommended)
    #   - opencv_face: face detection
    #   - opencv_motion: motion score 0-1
  vlm:
    enabled: true               # Enable VLM engine (requires OpenAI API, slower)
    model: gpt-4o-mini           # OpenAI vision model
    min_confidence: 0.55         # Confidence threshold for VLM events

# API runtime (api-reactive polling)
api:
  enabled: true                  # Enable API reactive features
  tick_interval_ms: 1000         # How often main loop checks for due fetches
  default_interval_ms: 30000     # Default API poll interval if state doesn't specify
  min_interval_ms: 1000          # Minimum allowed interval (rate limit protection)
  timeout: 15.0                  # HTTP request timeout in seconds
  cooldown_ms: 1000              # Minimum time between emitting same event

# Mic controller settings (on-demand: stream opens when needed)
mic:
  chunk_size: 1024               # Samples per audio frame
  buffer_max_chunks: 200         # Max frames in deque buffer (~4.6s at 44.1kHz)

# Audio LLM watcher runtime
# Continuous mic -> LLM analysis -> getData('audio') + events
# Two modes:
#   - transcript: audio → Whisper transcription → GPT-4o-mini text analysis (default)
#   - direct: audio → GPT-4o audio model (no transcription, faster but more API cost)
audio:
  enabled: true
  mode: direct               # 'transcript' or 'direct'
  model: gpt-4o-mini             # Model for text analysis (transcript mode)
  direct_audio_model: gpt-4o-mini-audio-preview-2024-12-17  # Model for direct audio
  interval_ms: 3000              # Milliseconds between analyses
  cooldown_ms: 1500              # Min time between same event
  max_transcript_chars: 20000    # Max transcript length (transcript mode)
  max_audio_bytes: 5000000       # Max audio size in bytes (direct mode, ~5MB)

# Volume watcher runtime (mic level processing)
# Writes to getData('volume') for state render code to use
volume:
  enabled: true
  interval_ms: 80
  smoothing_alpha: 0.35
  floor: 0.0
  ceiling: 1.0
